{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7254e3e-7c8b-4716-8ccd-661e9e66c003",
   "metadata": {},
   "source": [
    "#  Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1184ade4-b866-4790-a794-1df4295392d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>day_diff</th>\n",
       "      <th>helpful_yes</th>\n",
       "      <th>helpful_no</th>\n",
       "      <th>total_vote</th>\n",
       "      <th>score_pos_neg_diff</th>\n",
       "      <th>score_average_rating</th>\n",
       "      <th>wilson_lower_bound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>No issues.</td>\n",
       "      <td>2014-07-23</td>\n",
       "      <td>138</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0mie</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Purchased this for my device, it worked as adv...</td>\n",
       "      <td>2013-10-25</td>\n",
       "      <td>409</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1K3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>it works as expected. I should have sprung for...</td>\n",
       "      <td>2012-12-23</td>\n",
       "      <td>715</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1m2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This think has worked out great.Had a diff. br...</td>\n",
       "      <td>2013-11-21</td>\n",
       "      <td>382</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2&amp;amp;1/2Men</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Bought it with Retail Packaging, arrived legit...</td>\n",
       "      <td>2013-07-13</td>\n",
       "      <td>513</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  reviewerName  overall  \\\n",
       "0           0           NaN      4.0   \n",
       "1           1          0mie      5.0   \n",
       "2           2           1K3      4.0   \n",
       "3           3           1m2      5.0   \n",
       "4           4  2&amp;1/2Men      5.0   \n",
       "\n",
       "                                          reviewText  reviewTime  day_diff  \\\n",
       "0                                         No issues.  2014-07-23       138   \n",
       "1  Purchased this for my device, it worked as adv...  2013-10-25       409   \n",
       "2  it works as expected. I should have sprung for...  2012-12-23       715   \n",
       "3  This think has worked out great.Had a diff. br...  2013-11-21       382   \n",
       "4  Bought it with Retail Packaging, arrived legit...  2013-07-13       513   \n",
       "\n",
       "   helpful_yes  helpful_no  total_vote  score_pos_neg_diff  \\\n",
       "0            0           0           0                   0   \n",
       "1            0           0           0                   0   \n",
       "2            0           0           0                   0   \n",
       "3            0           0           0                   0   \n",
       "4            0           0           0                   0   \n",
       "\n",
       "   score_average_rating  wilson_lower_bound  \n",
       "0                   0.0                 0.0  \n",
       "1                   0.0                 0.0  \n",
       "2                   0.0                 0.0  \n",
       "3                   0.0                 0.0  \n",
       "4                   0.0                 0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download amazon reviews dataset from kaggle\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./data/amazon_reviews.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1e54bc-c64a-4cfd-a8ea-e48e997db6a8",
   "metadata": {},
   "source": [
    "#### Lowercasing\n",
    "- uniformity\n",
    "- no extra complexity due to uppercase and lowercase of same words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51c11fa2-76f0-4187-b8d6-ad80ccce5c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"this think has worked out great.had a diff. bran 64gb card and if went south after 3 months.this one has held up pretty well since i had my s3, now on my note3.*** update 3/21/14i've had this for a few months and have had zero issue's since it was transferred from my s3 to my note3 and into a note2. this card is reliable and solid!cheers!\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['reviewText'][3].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022a49c4-a574-4ce1-a1e4-03c7e72fd9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f329b5-7d0a-4ba4-9aa8-8f7f45762657",
   "metadata": {},
   "source": [
    "#### HTML Tags Removal\n",
    "- in case of web scraped data, html tags are present\n",
    "- not required for many applications in NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "304c5ba0-2bd8-4550-8778-4967158f8942",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"<a href=\"/wiki/Wikipedia:Purpose\" title=\"Wikipedia:Purpose\">Wikipedia's purpose</a> is to benefit readers by presenting information on all branches of <a href=\"/wiki/Knowledge\" title=\"Knowledge\">knowledge</a>. \n",
    "Hosted by the <a href=\"/wiki/Wikipedia:Wikimedia_Foundation\" title=\"Wikipedia:Wikimedia Foundation\">Wikimedia Foundation</a>, \n",
    "it consists of <a href=\"/wiki/Help:Editing\" title=\"Help:Editing\">freely editable</a> content, whose articles also have \n",
    "numerous links to guide readers towards more information.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "051a6433-5656-4c94-af20-dc8e33ee7453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Wikipedia's purpose is to benefit readers by presenting information on all branches of knowledge. \\nHosted by the Wikimedia Foundation, \\nit consists of freely editable content, whose articles also have \\nnumerous links to guide readers towards more information.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_html(text):\n",
    "    return re.sub('<.*?>', '', text)\n",
    "\n",
    "remove_html(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d608c100-21ee-469a-95ce-14070777f08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(removehtml) # apply on whole column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bf2aec-47e4-413c-93c0-c8d0e363a486",
   "metadata": {},
   "source": [
    "#### Renove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "349e27a5-e424-43ae-9a17-e940b855deef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' is just a fake website name'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_url(text):\n",
    "    return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "remove_url(\"https://www.dummyewebsite.com is just a fake website name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174d451c-a1be-44cd-bac1-28684f1f726b",
   "metadata": {},
   "source": [
    "#### Remove Punctuations\n",
    "- extra tokens of punctuations or sometimes punctuations are also part of tokenized words thus adding extra words\n",
    "- most of the time punctuations are not required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23f93107-4fa6-4cce-a68f-c56e45c9f6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 53.1 ms, sys: 0 ns, total: 53.1 ms\n",
      "Wall time: 51.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# slow execution\n",
    "import string, time\n",
    "exclude = string.punctuation\n",
    "\n",
    "def remove_punc(text):\n",
    "    for char in exclude:\n",
    "        text = text.replace(char, '')\n",
    "    return text\n",
    "\n",
    "df['reviewText'] = df['reviewText'].astype(str).apply(remove_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60e01b04-e023-4621-aba1-08e083bee02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.1 ms, sys: 2.9 ms, total: 42 ms\n",
      "Wall time: 40.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# faster\n",
    "def remove_punc_fast(text):\n",
    "    return text.translate(str.maketrans('', '', exclude))\n",
    "\n",
    "df['reviewText'] = df['reviewText'].astype(str).apply(remove_punc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa497915-a09e-4548-bdff-8a514f50bb3d",
   "metadata": {},
   "source": [
    "#### Chat Word Treatment\n",
    "- Get list of short forms and their expansions\n",
    "- create a dict of slang and its expansion\n",
    "- replace short slang with its expansion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb3e0b75-31b8-40b1-a934-d7397cbe9953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict of slang\n",
    "chat_words = \"\"\"AFAIK=As Far As I Know\n",
    "AFK=Away From Keyboard\n",
    "ASAP=As Soon As Possible\n",
    "ATK=At The Keyboard\n",
    "ATM=At The Moment\n",
    "A3=Anytime, Anywhere, Anyplace\n",
    "BAK=Back At Keyboard\n",
    "BBL=Be Back Later\n",
    "BBS=Be Back Soon\n",
    "BFN=Bye For Now\n",
    "B4N=Bye For Now\n",
    "BRB=Be Right Back\n",
    "BRT=Be Right There\n",
    "BTW=By The Way\n",
    "B4=Before\n",
    "B4N=Bye For Now\n",
    "CU=See You\n",
    "CUL8R=See You Later\n",
    "CYA=See You\n",
    "FAQ=Frequently Asked Questions\n",
    "FC=Fingers Crossed\n",
    "FWIW=For What It's Worth\n",
    "FYI=For Your Information\n",
    "GAL=Get A Life\n",
    "GG=Good Game\n",
    "GN=Good Night\n",
    "GMTA=Great Minds Think Alike\n",
    "GR8=Great!\n",
    "G9=Genius\n",
    "IC=I See\n",
    "ICQ=I Seek you (also a chat program)\n",
    "ILU=ILU: I Love You\n",
    "IMHO=In My Honest/Humble Opinion\n",
    "IMO=In My Opinion\n",
    "IOW=In Other Words\n",
    "IRL=In Real Life\n",
    "KISS=Keep It Simple, Stupid\n",
    "LDR=Long Distance Relationship\n",
    "LMAO=Laugh My A.. Off\n",
    "LOL=Laughing Out Loud\n",
    "LTNS=Long Time No See\n",
    "L8R=Later\n",
    "MTE=My Thoughts Exactly\n",
    "M8=Mate\n",
    "NRN=No Reply Necessary\n",
    "OIC=Oh I See\n",
    "PITA=Pain In The A..\n",
    "PRT=Party\n",
    "PRW=Parents Are Watching\n",
    "ROFL=Rolling On The Floor Laughing\n",
    "ROFLOL=Rolling On The Floor Laughing Out Loud\n",
    "ROTFLMAO=Rolling On The Floor Laughing My A.. Off\n",
    "SK8=Skate\n",
    "STATS=Your sex and age\n",
    "ASL=Age, Sex, Location\n",
    "THX=Thank You\n",
    "TTFN=Ta-Ta For Now!\n",
    "TTYL=Talk To You Later\n",
    "U=You\n",
    "U2=You Too\n",
    "U4E=Yours For Ever\n",
    "WB=Welcome Back\n",
    "WTF=What The F...\n",
    "WTG=Way To Go!\n",
    "WUF=Where Are You From?\n",
    "W8=Wait...\n",
    "7K=Sick:-D Laugher\n",
    "TFW = That feeling when. TFW internet slang often goes in a caption to an image.\n",
    "MFW = My face when\n",
    "MRW = My reaction when\n",
    "IFYP = I feel your pain\n",
    "LOL = Laughing out loud\n",
    "TNTL = Trying not to laugh\n",
    "JK = Just kidding\n",
    "IDC = I donâ€™t care\n",
    "ILY = I love you\n",
    "IMU = I miss you\n",
    "ADIH = Another day in hell\n",
    "IDC = I donâ€™t care\n",
    "ZZZ = Sleeping, bored, tired\n",
    "WYWH = Wish you were here\n",
    "TIME = Tears in my eyes\n",
    "BAE = Before anyone else\n",
    "FIMH = Forever in my heart\n",
    "BSAAW = Big smile and a wink\n",
    "BWL = Bursting with laughter\n",
    "LMAO = Laughing my a** off\n",
    "BFF = Best friends forever\n",
    "CSL = Canâ€™t stop laughing\"\"\"\n",
    "\n",
    "word_pairs = chat_words.split(\"\\n\")\n",
    "\n",
    "chat_dict = {}\n",
    "for pair in word_pairs:\n",
    "    key, value = pair.split('=')\n",
    "    chat_dict[key] = value\n",
    "# chat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1084457-5e53-42dd-9c82-c7f1ce5b881a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rolling On The Floor Laughing how this happened with you, Laughing Out Loud !'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chat_conversion(text):\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_dict:\n",
    "            new_text.append(chat_dict[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return ' '.join(new_text)\n",
    "\n",
    "chat_conversion(\"rofl how this happened with you, lol !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8b9c97-8d0c-45b1-b1f5-642be90a09a3",
   "metadata": {},
   "source": [
    "#### Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "733d1928-a148-4676-be9b-bdbf3b759c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'certain conditions during several generation'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def correct_spell(text):\n",
    "    text_blob = TextBlob(text)\n",
    "    return text_blob.correct().string\n",
    "\n",
    "correct_spell('certain conditionas duriing severall ggeneration')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0941b424-c28a-4f52-a54f-8d4de13c4aea",
   "metadata": {},
   "source": [
    "#### Remove stop words\n",
    "- nltk has list of stop words\n",
    "- stop words **need not be removed for POS tagging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3024e7b7-a4fd-46b9-bad1-09df40847d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/op/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bcea6bd8-b30a-477b-b8d4-0b18255def26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Its mini storage It doesnt  anything else    supposed  I purchased   add additional storage   Microsoft Surface Pro tablet   come  64  128 GB It    supposed   SanDisk   long standing reputation  speaks  '"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    x = new_text[:]\n",
    "    new_text.clear()\n",
    "    return ' '.join(x)\n",
    "\n",
    "remove_stopwords(df['reviewText'][5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3c2b38-1019-4fe3-bd5c-c3494f8c6dab",
   "metadata": {},
   "source": [
    "#### Handling Emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3bef38-20ad-4f33-bd8d-13abc7d7baf5",
   "metadata": {},
   "source": [
    "Two approaches to handle based on requirement:\n",
    "- remove emoji\n",
    "- replace with their name word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9076143c-1e2a-485f-be05-f50353e7ee60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Python is '"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove\n",
    "import re\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                              u\"\\U0001F600-\\U0001F64F\"   #emoticons\n",
    "                              u\"\\U0001F300-\\U0001F5FF\"   #symbols and pictographs\n",
    "                              u\"\\U0001F680-\\U0001F6FF\"   #transport and map symbols\n",
    "                              u\"\\U00012702-\\U000127B0\"   #flags\n",
    "                              u\"\\U000124C2-\\U0001F251\"\n",
    "                              \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "remove_emoji('Python is ðŸ”¥')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d9d470-26a2-4fd7-8d74-36d0a1d341bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f9fa2b57-d380-4a8e-bf31-b84e9b942d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is :fire:\n"
     ]
    }
   ],
   "source": [
    "# replace\n",
    "import emoji\n",
    "\n",
    "print(emoji.demojize('Python is ðŸ”¥'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef5edf3-aae9-413a-8029-66da5e3018a7",
   "metadata": {},
   "source": [
    "### Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca861324-fa9b-4de4-950a-4ba3763d258a",
   "metadata": {},
   "source": [
    "##### basic using split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "74fcdf9f-d302-47a5-8890-21cb43f7513e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Consistency', 'is', 'a', 'crucial', 'aspect', 'of', 'success', 'in', 'many', 'areas', 'of', 'life,', 'including', 'personal', 'growth,', 'professional', 'development,', 'and', 'relationships'], ['Consistency', 'means', 'performing', 'actions', 'and', 'making', 'decisions', 'in', 'a', 'predictable', 'and', 'reliable', 'manner,', 'over', 'time', 'and', 'across', 'different', 'situations'], ['By', 'being', 'consistent,', 'we', 'build', 'trust', 'and', 'credibility', 'with', 'others,', 'and', 'we', 'also', 'create', 'a', 'sense', 'of', 'stability', 'and', 'security', 'for', 'ourselves'], []]\n"
     ]
    }
   ],
   "source": [
    "paragraph = \"\"\"Consistency is a crucial aspect of success in many areas of life, including personal growth, professional development, and relationships. Consistency means performing actions and making decisions in a predictable and reliable manner, over time and across different situations. By being consistent, we build trust and credibility with others, and we also create a sense of stability and security for ourselves.\"\"\"\n",
    "\n",
    "sentences = paragraph.split('.')\n",
    "\n",
    "words = [sentence.split() for sentence in sentences]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da162e77-4d7c-4a25-a51d-e0ac9295e553",
   "metadata": {},
   "source": [
    "##### using regEx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "333f6236-b9cf-4a73-9acb-8f68d0dd1c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Consistency', 'is', 'a', 'crucial', 'aspect', 'of', 'success', 'in', 'many', 'areas', 'of', 'life,', 'including', 'personal', 'growth,', 'professional', 'development,', 'and', 'relationships', '', 'Consistency', 'means', 'performing', 'actions', 'and', 'making', 'decisions', 'in', 'a', 'predictable', 'and', 'reliable', 'manner,', 'over', 'time', 'and', 'across', 'different', 'situations', '', 'By', 'being', 'consistent,', 'we', 'build', 'trust', 'and', 'credibility', 'with', 'others,', 'and', 'we', 'also', 'create', 'a', 'sense', 'of', 'stability', 'and', 'security', 'for', 'ourselves', '']\n"
     ]
    }
   ],
   "source": [
    "sentences = re.compile('[ .!?]').split(paragraph)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc0a492-3882-4598-9603-848fb51395a2",
   "metadata": {},
   "source": [
    "##### using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "268d1e47-5018-4021-9236-3cd485b0abfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Consistency also helps us to build habits and routines that support our well-being and success.', 'By consistently practicing healthy habits, such as exercise, meditation, and healthy eating, we can improve our physical and mental health and increase our productivity and creativity.']\n",
      "['Consistency', 'also', 'helps', 'us', 'to', 'build', 'habits', 'and', 'routines', 'that', 'support', 'our', 'well-being', 'and', 'success', '.']\n",
      "['By', 'consistently', 'practicing', 'healthy', 'habits', ',', 'such', 'as', 'exercise', ',', 'meditation', ',', 'and', 'healthy', 'eating', ',', 'we', 'can', 'improve', 'our', 'physical', 'and', 'mental', 'health', 'and', 'increase', 'our', 'productivity', 'and', 'creativity', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "para1 = \"\"\"Consistency also helps us to build habits and routines that support our well-being and success. By consistently practicing healthy habits, such as exercise, meditation, and healthy eating, we can improve our physical and mental health and increase our productivity and creativity.\"\"\"\n",
    "\n",
    "sentences = sent_tokenize(para1)\n",
    "print(sentences)\n",
    "\n",
    "for sent in sentences:\n",
    "    print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eb70a1-27b6-4b38-84c4-4daa2f44b6c6",
   "metadata": {},
   "source": [
    "##### using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bdc5a5-63ac-4e7b-9289-5ab692235790",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3398dfc3-42af-483f-b02d-a783be93cb93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7a3cad5e-af19-4c28-a84f-51cd152a8a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python\n",
      "is\n",
      "a\n",
      "popular\n",
      "programming\n",
      "language\n",
      "for\n",
      "NLP\n",
      ",\n",
      "with\n",
      "libraries\n",
      "such\n",
      "as\n",
      "NLTK\n",
      ",\n",
      "Spacy\n",
      ",\n",
      "and\n",
      "Gensim\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sent1 = \"Natural Language Processing (NLP) is a field of computer science that focuses on the interaction between computers and human language.\"\n",
    "sent2 = \"Python is a popular programming language for NLP, with libraries such as NLTK, Spacy, and Gensim.\"\n",
    "sent3 = \"NLP tasks include text classification, sentiment analysis, and named entity recognition.\"\n",
    "sent4 = \"In NLP, text data is preprocessed using techniques such as tokenization, stopword removal, and stemming.\"\n",
    "\n",
    "doc1 = nlp(sent1)\n",
    "doc2 = nlp(sent2)\n",
    "doc3 = nlp(sent3)\n",
    "doc4 = nlp(sent4)\n",
    "\n",
    "for token in doc2:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ac3406-cc51-414c-a6cf-6465151c9304",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33423b77-d53b-4786-8f82-4897f6a6d7a3",
   "metadata": {},
   "source": [
    "##### Inflection\n",
    "- Modification of a word to express diferent grammaticalcategories such as tense case, voice, aspect, person, number, gender and mood\n",
    "    - Ex: walk, walking, walked\n",
    "\n",
    "#### Stemming\n",
    "- Reducing inflection in words to their root forms such mapping group of words to the same stem even if **the stem itself may not be a valid word in the language**.\n",
    "- extracting root words\n",
    "- used in Information Retrieval systems - search engines\n",
    "\n",
    "- Different stemming algos:\n",
    "    - **Porter Stemmer**:\n",
    "        - Used in English text\n",
    "    - **Snowball Stemmer**:\n",
    "        - Used in other languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec99e340-c865-4719-bd25-829fae6eb81b",
   "metadata": {},
   "source": [
    "##### Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ae85726e-4ea7-4089-becb-a5e136b0e14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk walk walk undoabl'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stem_words(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])\n",
    "\n",
    "sample = \"Walk walked walking undoable\"\n",
    "\n",
    "stem_words(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c538bbe-889d-46b4-81d5-fc31d16dbbb2",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "- Same as stemming but the stem is always a valid language word\n",
    "- Slow process\n",
    "- If user is to be shown output, consider lemmatization else stemming\n",
    "- Root word is called 'lemma'. **Lemma** is the canonical form, dict form or citation form of a set of words.\n",
    "- works by searching in dictionary and finding the exact lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5c805515-ec82-4bbb-af32-590a45c6e1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/op/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ac2d1e76-4834-4244-abf1-0c4e341dacb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "------------------------------\n",
      "He                  He                  \n",
      "was                 be                  \n",
      "running             run                 \n",
      "for                 for                 \n",
      "5                   5                   \n",
      "kilometers          kilometers          \n",
      "he                  he                  \n",
      "got                 get                 \n",
      "tired               tire                \n",
      "and                 and                 \n",
      "lied                lie                 \n",
      "down                down                \n",
      "on                  on                  \n",
      "the                 the                 \n",
      "ground              grind               \n",
      "for                 for                 \n",
      "20                  20                  \n",
      "minutes             minutes             \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running for 5 kilometers, he got tired and lied down on the ground for 20 minutes\"\n",
    "punctuations = \"?.!,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "print(\"{0:20}{1:20}\".format(\"Word\", \"Lemma\"))\n",
    "print(\"------\"*5)\n",
    "for word in sentence_words:\n",
    "    print(\"{0:20}{1:20}\".format(word, wordnet_lemmatizer.lemmatize(word, pos='v')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
