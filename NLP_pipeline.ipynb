{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b018c4b6-dd02-4d86-bdcb-554bb364efce",
   "metadata": {},
   "source": [
    "# Natural Language Processing(NLP) - Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae75c8c8-f545-428f-a60f-1cee1cad285b",
   "metadata": {},
   "source": [
    "NLP Pipeline consists of following steps:\n",
    "1. Data Acquisition\n",
    "2. Text Preparation\n",
    "    * Text Cleanup\n",
    "    * Basic Preprocessing\n",
    "    * Advance Preprocessing\n",
    "4. Feature Engineering\n",
    "5. Modelling\n",
    "   * Model Bulding\n",
    "   * Evaluation\n",
    "7. Deployment\n",
    "   * Deployment\n",
    "   * Monitoring\n",
    "   * Model Update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8d462a-03a0-4254-82da-41ba9670a0b5",
   "metadata": {},
   "source": [
    "## 1. Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3b1ffe-781f-4772-b624-5e28b9276d1d",
   "metadata": {},
   "source": [
    "Three possible scenario for task at hand:\n",
    "- Available\n",
    "- Available to others \n",
    "- Does not exist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a23e0b-de9c-4065-86cf-893b010e9bb5",
   "metadata": {},
   "source": [
    "![Data Acquisition](./figures/data_acquisition.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161eb02e-f8ee-437e-98b8-940b43690abd",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bb3c06-c60e-46b2-8e32-acb9f113dc72",
   "metadata": {},
   "source": [
    "![Data Acquisition](./figures/data_preprocessing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbd8193-d391-40df-8290-6b2308807025",
   "metadata": {},
   "source": [
    "#### HTML tag removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "929ed509-cc7a-4902-8e83-8f95e8c7ca8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"<a href=\"/wiki/Wikipedia:Purpose\" title=\"Wikipedia:Purpose\">Wikipedia's purpose</a> is to benefit readers by presenting information on all branches of <a href=\"/wiki/Knowledge\" title=\"Knowledge\">knowledge</a>. \n",
    "Hosted by the <a href=\"/wiki/Wikipedia:Wikimedia_Foundation\" title=\"Wikipedia:Wikimedia Foundation\">Wikimedia Foundation</a>, \n",
    "it consists of <a href=\"/wiki/Help:Editing\" title=\"Help:Editing\">freely editable</a> content, whose articles also have \n",
    "numerous links to guide readers towards more information.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcd3454c-0f27-4e83-8ce1-fa99e6db3871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def striphtml(data):\n",
    "    return re.sub('<.*?>', '', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c24cc07b-90f6-49a7-af04-5bc6ceb933cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Wikipedia's purpose is to benefit readers by presenting information on all branches of knowledge. \\nHosted by the Wikimedia Foundation, \\nit consists of freely editable content, whose articles also have \\nnumerous links to guide readers towards more information.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "striphtml(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8148b218-938e-498a-9d19-91d38232f87b",
   "metadata": {},
   "source": [
    "#### Emoji removal - Unicode Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c71bd44f-a782-4a2f-99b3-fec05f629bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_text = \"Gratitude is key. ðŸ™ Appreciate what you have: home ðŸ , family ðŸ‘¨â€ðŸ‘©â€ðŸ‘§, work ðŸ’», food ðŸ², nature ðŸŒ³, friends ðŸ¤. Count your blessings daily. Stay thankful. ðŸ’–\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b6ec151-19b6-4a76-bbdd-7495d64d6a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Gratitude is key. \\xf0\\x9f\\x99\\x8f Appreciate what you have: home \\xf0\\x9f\\x8f\\xa0, family \\xf0\\x9f\\x91\\xa8\\xe2\\x80\\x8d\\xf0\\x9f\\x91\\xa9\\xe2\\x80\\x8d\\xf0\\x9f\\x91\\xa7, work \\xf0\\x9f\\x92\\xbb, food \\xf0\\x9f\\x8d\\xb2, nature \\xf0\\x9f\\x8c\\xb3, friends \\xf0\\x9f\\xa4\\x9d. Count your blessings daily. Stay thankful. \\xf0\\x9f\\x92\\x96'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_text.encode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321179b8-8b81-435d-ab8d-e6f43eea25ad",
   "metadata": {},
   "source": [
    "#### Spell Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7000c2fd-f958-487f-8004-69b8263a29fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting textblob\n",
      "  Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m626.3/626.3 KB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nltk>=3.8\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting joblib\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk>=3.8->textblob) (8.0.3)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Collecting regex>=2021.8.3\n",
      "  Using cached regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, nltk, textblob\n",
      "Successfully installed joblib-1.3.2 nltk-3.8.1 regex-2023.12.25 textblob-0.18.0.post0 tqdm-4.66.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7709f48d-146b-41e6-b29c-20b8e2c886ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_text = 'ceartain aonditiona during seveerel ggenrations are modifiedd in the saame manner' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d49d414-a828-4cf3-a5fa-7e0ce36142a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"certain condition during several generations are modified in the same manner\")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "textBlob = TextBlob(incorrect_text)\n",
    "\n",
    "textBlob.correct()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abde7fb-9890-4c9b-b4fa-8dc372baa90e",
   "metadata": {},
   "source": [
    "### Basic Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27186988-2122-4793-aae4-367ef236f052",
   "metadata": {},
   "source": [
    "#### Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cb7ef70-1ea2-4ab6-be18-b255b06af566",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/op/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cecb96a9-25ef-4bde-90aa-3376f9edec15",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed bibendum turpis a enim imperdiet, sit amet efficitur ipsum pharetra. In eu ipsum non nisi tincidunt consectetur. Donec vitae nisi vel metus eleifend tempor. Curabitur fringilla, nibh non ultrices volutpat, purus turpis lacinia nunc, vel aliquet dolor ipsum et nisi. Donec eget nibh vel nisl consectetur sodales.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28328f54-1236-4abb-bda8-57d9b573b796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem ipsum dolor sit amet, consectetur adipiscing elit.',\n",
       " 'Sed bibendum turpis a enim imperdiet, sit amet efficitur ipsum pharetra.',\n",
       " 'In eu ipsum non nisi tincidunt consectetur.',\n",
       " 'Donec vitae nisi vel metus eleifend tempor.',\n",
       " 'Curabitur fringilla, nibh non ultrices volutpat, purus turpis lacinia nunc, vel aliquet dolor ipsum et nisi.',\n",
       " 'Donec eget nibh vel nisl consectetur sodales.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "sentences = sent_tokenize(sample_text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79fea7e7-2468-4e9a-9669-d5905c24466b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lorem', 'ipsum', 'dolor', 'sit', 'amet', ',', 'consectetur', 'adipiscing', 'elit', '.']\n",
      "['Sed', 'bibendum', 'turpis', 'a', 'enim', 'imperdiet', ',', 'sit', 'amet', 'efficitur', 'ipsum', 'pharetra', '.']\n",
      "['In', 'eu', 'ipsum', 'non', 'nisi', 'tincidunt', 'consectetur', '.']\n",
      "['Donec', 'vitae', 'nisi', 'vel', 'metus', 'eleifend', 'tempor', '.']\n",
      "['Curabitur', 'fringilla', ',', 'nibh', 'non', 'ultrices', 'volutpat', ',', 'purus', 'turpis', 'lacinia', 'nunc', ',', 'vel', 'aliquet', 'dolor', 'ipsum', 'et', 'nisi', '.']\n",
      "['Donec', 'eget', 'nibh', 'vel', 'nisl', 'consectetur', 'sodales', '.']\n"
     ]
    }
   ],
   "source": [
    "for sent in sentences:\n",
    "    print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61228ab-4bbb-44be-af8d-5b39601297d2",
   "metadata": {},
   "source": [
    "_Other techniques we will code later_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669dcd5b-b83e-443c-affd-ff58858ba74b",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad7efd3-31ea-49f6-963d-e584ccb69596",
   "metadata": {},
   "source": [
    "Converting textual data into numbers so that it can be fed to model training and testing.\n",
    "\n",
    "Two high level approaches based on model building algorithms\n",
    "- ML Algorithms: features need to be generated manually based on domain knowledge\n",
    "- DL Algorithms: features are generated by algorithms\n",
    "\n",
    "Both have its own advantages and disadvantages:\n",
    "- ML Algorithms\n",
    "    - Interpretable results\n",
    "    - Domain knowledge required\n",
    "    - Features generated may not be robust\n",
    "- DL Algorithms\n",
    "    - Features are generated by algorithms\n",
    "    - Less interpretable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273092cd-222e-4414-9e88-c5251638d1bd",
   "metadata": {},
   "source": [
    "## 4. Modelling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02ca2c7-3278-4285-a96c-e8ca6545fc1d",
   "metadata": {},
   "source": [
    "![Data Acquisition](./figures/modelling.png)\n",
    "<center>Modelling Phase of NLP Pipeline</center>\n",
    "\n",
    "- Based on data avilability and nature of problem, we chose among different approaches to build a model.\n",
    "- `Intrinsic Evaluation` helps to evaluate model performance like accuracy, perplexityetc.\n",
    "- `Extrinsic Evaluation` helps in evaluating model performane in business settings like how many times user are using suggestions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb5586f-f8fb-4e74-a9ff-28544f9da1e9",
   "metadata": {},
   "source": [
    "## 5. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39102a86-1ba3-4b8c-aa18-745dd4cfcf0c",
   "metadata": {},
   "source": [
    "Three phases of deployment are:\n",
    "- **Deploying**: Depending on application usecase of model different approaches are used.\n",
    "  Ex: API(Microservices), chatbot, App integration\n",
    "- **Monitoring**: Tracking model performance through dashboards.\n",
    "- **Model Update**: Based on new data availability or to increase robustness, updates are done. Online Learning, model trains on the server itself as the data comes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
