{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmLKVCVBIUsA"
   },
   "source": [
    "## Preprocessing Assignment\n",
    "1. [X] Fetch API and create a dataset of movie name, overview, genres\n",
    "2. [X] Apply suitable preprocessing on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CiDc0hiPJCuE"
   },
   "source": [
    "### 1. Fetching data from API and creating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Il3fZhSwaOZH",
    "outputId": "9abd19d9-6c67-433a-87ea-2587ddad59d7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns=['title', 'overview', 'genre_ids'])\n",
    "\n",
    "# url of movies api\n",
    "url = \"https://api.themoviedb.org/3/movie/top_rated?api_key=8265bd1679663a7ea12ac168da84d2e8&language=en-US&page=\"\n",
    "\n",
    "# fetch for 470 pages\n",
    "for page in tqdm(range(470)):\n",
    "  # GET request to api url\n",
    "  response = requests.get(f'{url}{page+1}')\n",
    "\n",
    "  # parse the response as JSON\n",
    "  data = json.loads(response.text)\n",
    "\n",
    "  # get results from JSON\n",
    "  results = data[\"results\"]\n",
    "\n",
    "  for result in results:\n",
    "    # extract title, overview, and genre ids from the result\n",
    "    row = {\"title\": result[\"title\"], \"overview\": result[\"overview\"], \"genre_ids\": result[\"genre_ids\"]}\n",
    "\n",
    "    # create a new df and concat in df\n",
    "    df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "e4C1Xm_WaWXI",
    "outputId": "24af5465-5f98-4abf-94c4-9fbe9874c7de"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFouZXvCJPbP"
   },
   "source": [
    "#### Replace genre_ids with genre names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ms1LBN-JeA1_",
    "outputId": "2fb7011b-dfa0-4abf-f6c1-0664077a3437"
   },
   "outputs": [],
   "source": [
    "# URL of genre ids and its names\n",
    "url = \"https://api.themoviedb.org/3/genre/movie/list?api_key=8265bd1679663a7ea12ac168da84d2e8&language=en-US\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "data = json.loads(response.text)\n",
    "\n",
    "results_dict = data[\"genres\"]\n",
    "# print(results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DuU9j9dGd9K9"
   },
   "outputs": [],
   "source": [
    "# function to eturn genre names for ids\n",
    "def get_genre_name(genre_ids, results_dict):\n",
    "  genre_names = []\n",
    "  for genre_id in genre_ids:\n",
    "    for result in results_dict:\n",
    "      if result[\"id\"] == genre_id:\n",
    "        genre_names.append(result[\"name\"])\n",
    "  return ','.join(genre_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tnR-cAHQgnbT"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df['genres'] = df.apply(lambda x: get_genre_name(x['genre_ids'], results_dict), axis=1)\n",
    "df.drop('genre_ids', axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df to csv for later use\n",
    "df.to_csv(\"./data/movies.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtLSr807Jc0d"
   },
   "source": [
    "### 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which preprocessing operation needs to be done is guided by objective of our task at hand. Since the dataset is created from API response, the data is already clean, so many of preprocessing tasks like HTML Tag removal, URL removal, Chat word treatment, Spelling Correction, Emoji handling are not required to apply here. \n",
    "\n",
    "Preprocessing techniques generally needed for this dataset are (subject to end goal objective) -\n",
    "- [x] Lowercasing\n",
    "- [x] Punctuation Removal\n",
    "- [x] Stop Word Removal\n",
    "- [x] Tokenisation\n",
    "- [x] Stemming\n",
    "- [x] Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GiqP6NzEJiIa"
   },
   "outputs": [],
   "source": [
    "# read saved csv\n",
    "df = pd.read_csv(\"./data/movies.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['overview'] = df['overview'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Punctuation removal\n",
    "**Caution:** If we remove punctuation, all sentence will be merged in as a single sentence, so we can't apply sentence tokenisation later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "exclude = string.punctuation\n",
    "\n",
    "def remove_punc_fast(text):\n",
    "    return text.translate(str.maketrans('', '', exclude))\n",
    "\n",
    "df['overview'] = df['overview'].astype(str).apply(remove_punc_fast)\n",
    "df['overview'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    x = new_text[:]\n",
    "    new_text.clear()\n",
    "    return ' '.join(x)\n",
    "\n",
    "df['overview'] = df['overview'].astype(str).apply(remove_stopwords)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "- only word tokenisation as we have removed punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "df['token_words'] = df['overview'].apply(lambda x: word_tokenize(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Porter Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stem_words(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])\n",
    "\n",
    "df['stem_words'] = df['overview'].astype(str).apply(stem_words)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_words(words):\n",
    "    lemmatized = []\n",
    "    for word in words:\n",
    "        lemmatized.append(wordnet_lemmatizer.lemmatize(word, pos='v'))\n",
    "    return \" \".join(lemmatized)\n",
    "\n",
    "df['lemma_words'] = df['token_words'].apply(lemmatize_words)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a df to see where lemma words are different from stem_words\n",
    "df['diff_cols'] = df.apply(lambda x: x['stem_words'] != x['lemma_words'], axis=1)\n",
    "diff_df = df[df['diff_cols'] == True]\n",
    "df.drop('diff_cols', axis=1, inplace=True)\n",
    "diff_df[['stem_words', 'lemma_words']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unapplicable Preprocessing Operations\n",
    "- HTML tag removal\n",
    "- URL removal\n",
    "- Chat word treatment\n",
    "- Spelling correction\n",
    "- Emoji handling\n",
    "\n",
    "Here these preprocessing techniques are unapplicable still for learning purpose we can apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read saved csv\n",
    "df = pd.read_csv(\"./data/movies.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML tags removal\n",
    "import re\n",
    "\n",
    "def remove_html(text):\n",
    "    return re.sub(r'<.*?>', '', text)\n",
    "\n",
    "df['overview'] = df['overview'].astype(str).apply(remove_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# URL removal\n",
    "def remove_url(text):\n",
    "    return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "df['overview'] = df['overview'].apply(remove_url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat word treatment\n",
    "\n",
    "# create dict of slang\n",
    "chat_words = \"\"\"AFAIK=As Far As I Know\n",
    "AFK=Away From Keyboard\n",
    "ASAP=As Soon As Possible\n",
    "ATK=At The Keyboard\n",
    "ATM=At The Moment\n",
    "A3=Anytime, Anywhere, Anyplace\n",
    "BAK=Back At Keyboard\n",
    "BBL=Be Back Later\n",
    "BBS=Be Back Soon\n",
    "BFN=Bye For Now\n",
    "B4N=Bye For Now\n",
    "BRB=Be Right Back\n",
    "BRT=Be Right There\n",
    "BTW=By The Way\n",
    "B4=Before\n",
    "B4N=Bye For Now\n",
    "CU=See You\n",
    "CUL8R=See You Later\n",
    "CYA=See You\n",
    "FAQ=Frequently Asked Questions\n",
    "FC=Fingers Crossed\n",
    "FWIW=For What It's Worth\n",
    "FYI=For Your Information\n",
    "GAL=Get A Life\n",
    "GG=Good Game\n",
    "GN=Good Night\n",
    "GMTA=Great Minds Think Alike\n",
    "GR8=Great!\n",
    "G9=Genius\n",
    "IC=I See\n",
    "ICQ=I Seek you (also a chat program)\n",
    "ILU=ILU: I Love You\n",
    "IMHO=In My Honest/Humble Opinion\n",
    "IMO=In My Opinion\n",
    "IOW=In Other Words\n",
    "IRL=In Real Life\n",
    "KISS=Keep It Simple, Stupid\n",
    "LDR=Long Distance Relationship\n",
    "LMAO=Laugh My A.. Off\n",
    "LOL=Laughing Out Loud\n",
    "LTNS=Long Time No See\n",
    "L8R=Later\n",
    "MTE=My Thoughts Exactly\n",
    "M8=Mate\n",
    "NRN=No Reply Necessary\n",
    "OIC=Oh I See\n",
    "PITA=Pain In The A..\n",
    "PRT=Party\n",
    "PRW=Parents Are Watching\n",
    "ROFL=Rolling On The Floor Laughing\n",
    "ROFLOL=Rolling On The Floor Laughing Out Loud\n",
    "ROTFLMAO=Rolling On The Floor Laughing My A.. Off\n",
    "SK8=Skate\n",
    "STATS=Your sex and age\n",
    "ASL=Age, Sex, Location\n",
    "THX=Thank You\n",
    "TTFN=Ta-Ta For Now!\n",
    "TTYL=Talk To You Later\n",
    "U=You\n",
    "U2=You Too\n",
    "U4E=Yours For Ever\n",
    "WB=Welcome Back\n",
    "WTF=What The F...\n",
    "WTG=Way To Go!\n",
    "WUF=Where Are You From?\n",
    "W8=Wait...\n",
    "7K=Sick:-D Laugher\n",
    "TFW = That feeling when. TFW internet slang often goes in a caption to an image.\n",
    "MFW = My face when\n",
    "MRW = My reaction when\n",
    "IFYP = I feel your pain\n",
    "LOL = Laughing out loud\n",
    "TNTL = Trying not to laugh\n",
    "JK = Just kidding\n",
    "IDC = I don’t care\n",
    "ILY = I love you\n",
    "IMU = I miss you\n",
    "ADIH = Another day in hell\n",
    "IDC = I don’t care\n",
    "ZZZ = Sleeping, bored, tired\n",
    "WYWH = Wish you were here\n",
    "TIME = Tears in my eyes\n",
    "BAE = Before anyone else\n",
    "FIMH = Forever in my heart\n",
    "BSAAW = Big smile and a wink\n",
    "BWL = Bursting with laughter\n",
    "LMAO = Laughing my a** off\n",
    "BFF = Best friends forever\n",
    "CSL = Can’t stop laughing\"\"\"\n",
    "\n",
    "word_pairs = chat_words.split(\"\\n\")\n",
    "\n",
    "chat_dict = {}\n",
    "for pair in word_pairs:\n",
    "    key, value = pair.split('=')\n",
    "    chat_dict[key] = value\n",
    "# chat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def chat_conversion(text):\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_dict:\n",
    "            new_text.append(chat_dict[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return ' '.join(new_text)\n",
    "\n",
    "df['overview'] = df['overview'].astype(str).apply(chat_conversion)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Spelling Correction\n",
    "from textblob import TextBlob\n",
    "\n",
    "def correct_spell(text):\n",
    "    text_blob = TextBlob(text)\n",
    "    return text_blob.correct().string\n",
    "\n",
    "df['overview'] = df['overview'].astype(str).apply(correct_spell)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace emoji - a better choice here\n",
    "import emoji\n",
    "\n",
    "df['overview'] = df['overview'].apply(lambda x: emoji.demojize(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Errors:\n",
    "- Faced some SSL Error in Jupyter Lab while fetching API\n",
    "\n",
    "    **SSLEOFError: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)**\n",
    "  \n",
    "    So created the dataset by running the data creation part of code in `Colab` and downloaded CSV file."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
